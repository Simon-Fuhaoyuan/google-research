# Currently, we only focus on RoboTube-DrawerClosing task.

# Training with env reward
python rl_robotube_env_reward.py --env DrawerClosingStructured-v1 --seeds 1,2,3,4,5
python rl_robotube_env_reward.py --env DrawerClosingUR5Structured-v1 --seeds 1,2,3,4,5


# GC experiment

# Train with learned reward, using small model
python rl_robotube_learned_reward.py --env DrawerClosingUR5Structured-v1 --pretrain small --seeds 1,2,3,4,5
python rl_robotube_learned_reward.py --env DrawerClosingUR5Structured-v1 --pretrain small --seeds 1,2,3,4,5 --parallel

# Train with learned reward, using large model
python rl_robotube_learned_reward.py --env DrawerClosingUR5Structured-v1 --pretrain large --seeds 1,2,3,4,5
python rl_robotube_learned_reward.py --env DrawerClosingUR5Structured-v1 --pretrain large --seeds 1,2,3,4,5 --parallel


# TCN experiment

python rl_robotube_tcn.py --env DrawerClosingUR5Structured-v1
python rl_robotube_tcn.py --env DrawerClosingUR5Structured-v1 --parallel


# Plot multiple exps in a single graph
python plot.py --y_label "Success Rate" --log_dir logs/DrawerClosingStructured-v1_20220515_23_23_27

# Plot a specific training
python plot.py --y_label "Success Rate" --log_dir logs/DrawerClosingStructured-v1_20220515_23_23_27/3/
